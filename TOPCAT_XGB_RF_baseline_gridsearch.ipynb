{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brierskillscore(y_pred, y_true):\n",
    "    cls_num_list = np.unique(y_true, return_counts=True)[1]\n",
    "    ratio = float(cls_num_list[1]) / np.sum(cls_num_list)\n",
    "    y_true, y_pred = y_true.reshape(1,-1).squeeze(), y_pred.reshape(1,-1).squeeze()\n",
    "    predictions = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    predictions_ref = [ratio] * len(predictions)\n",
    "    testy = y_true\n",
    "    BS = [brier_score_loss(testy, [y for x in range(len(testy))]) for y in predictions]\n",
    "    BS_ref = [brier_score_loss(testy, [y for x in range(len(testy))]) for y in predictions_ref]\n",
    "    BS_skill = 1 - np.array(BS)/np.array(BS_ref)\n",
    "\n",
    "\n",
    "    return BS_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci(X):\n",
    "    mean = np.mean(X)\n",
    "    bound = 1.96*np.std(X)/np.sqrt(len(X))\n",
    "    print(f'{mean:.3f}, [{mean-bound:.3f}, {mean+bound:.3f}]')\n",
    "    #return mean, mean-bound, mean+bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = [\n",
    "    'death',\n",
    "    'cvd_death',\n",
    "    'time_death',\n",
    "    'anyhosp',\n",
    "    'time_anyhosp',\n",
    "    'hfhosp',\n",
    "    'time_hfhosp',\n",
    "    'abortedca',\n",
    "    'time_abortedca',\n",
    "    'mi',\n",
    "    'time_mi',\n",
    "    'stroke',\n",
    "    'time_stroke',\n",
    "    'primary_ep',\n",
    "    'time_primary_ep'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_drops = [\n",
    "     'mr_mod',\n",
    "     'symp_bur_score',\n",
    "     'tot_symp_score',\n",
    "     'self_eff_score',\n",
    "     'qol_score',\n",
    "     'soc_limit_score',\n",
    "     'overall_sum_score',\n",
    "     'clin_sum_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_cat_cols = [\n",
    "    'GLUCOSE_FAST',\n",
    "    'GLUCOSE_RAND',\n",
    "    'CO2_mmolL',\n",
    "    'GLUCOSE_mgdL',\n",
    "    'WBC_kuL',\n",
    "    'HCT_p',\n",
    "    'HB_gdL',\n",
    "    'PLT_kuL',\n",
    "    'ALP_UL',\n",
    "    'TBILI_mgdL',\n",
    "    'ALB_gdL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_cols = ['BNP_VAL',\n",
    " 'age_entry',\n",
    " 'EF',\n",
    " 'visit_dt1_hf',\n",
    " 'chfdc_dt3',\n",
    " 'mi_dt3',\n",
    " 'stroke_dt3',\n",
    " 'cabg_dt3',\n",
    " 'pci_dt3',\n",
    " 'DM_AGE_YR',\n",
    " 'DM_DUR_YR',\n",
    " 'cigs',\n",
    " 'SMOKE_YRS',\n",
    " 'QUIT_YRS',\n",
    " 'HEAVY_MIN',\n",
    " 'HEAVY_WK',\n",
    " 'MED_WK',\n",
    " 'MED_MIN',\n",
    " 'LIGHT_WK',\n",
    " 'LIGHT_MIN',\n",
    " 'metsperweek',\n",
    " 'cooking_salt_score',\n",
    " 'height',\n",
    " 'weight',\n",
    " 'waistc',\n",
    " 'HR',\n",
    " 'SBP',\n",
    " 'DBP',\n",
    " 'CR_mgdl',\n",
    " 'gfr',\n",
    " 'labs_dt1',\n",
    " 'NA_mmolL',\n",
    " 'K_mmolL',\n",
    " 'CL_mmolL',\n",
    " 'BUN_mgdL',\n",
    " 'ALT_UL',\n",
    " 'AST_UL',\n",
    " 'urine_val_mgg',\n",
    " 'QRS_DUR',\n",
    " 'CR_mgdL',\n",
    " 'BMI'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = pd.read_csv(\n",
    "    '/data/datasets/topcat/py_cleaned_data/TOPCAT_final_2_25_2020.csv',\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=3, Cut Off=3\n",
      "Fold 0: XGB AUC=0.7262759924385633\n",
      "\tRF AUC=0.7366099558916194\n",
      "Fold 1: XGB AUC=0.7456836798991809\n",
      "\tRF AUC=0.7601764335223692\n",
      "Fold 2: XGB AUC=0.7710399186371727\n",
      "\tRF AUC=0.7623951182303585\n",
      "Fold 3: XGB AUC=0.7482097186700768\n",
      "\tRF AUC=0.7599104859335039\n",
      "Fold 4: XGB AUC=0.7358056265984656\n",
      "\tRF AUC=0.7381074168797953\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [3]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier()\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=3, Cut Off=3\n",
      "Fold 0: Best parameters: {'gamma': 0, 'learning_rate': 0.15, 'max_depth': 8, 'n_estimators': 200}\n",
      "\tWith score0.724007561436673\n",
      "Fold 1: Best parameters: {'gamma': 1, 'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 40}\n",
      "\tWith score0.7282923755513547\n",
      "Fold 2: Best parameters: {'gamma': 0, 'learning_rate': 0.15, 'max_depth': 8, 'n_estimators': 160}\n",
      "\tWith score0.7524790236460717\n",
      "Fold 3: Best parameters: {'gamma': 1, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 80}\n",
      "\tWith score0.7485933503836316\n",
      "Fold 4: Best parameters: {'gamma': 0, 'learning_rate': 0.15, 'max_depth': 2, 'n_estimators': 120}\n",
      "\tWith score0.7493606138107417\n"
     ]
    }
   ],
   "source": [
    "clf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [3]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            parameters = {\n",
    "                'max_depth': range (2, 10, 2),\n",
    "                'n_estimators': range(40, 220, 40),\n",
    "                'learning_rate': [0.2, 0.15, 0.1],\n",
    "                'gamma': [0, 1, 2, 5]\n",
    "            }\n",
    "            \n",
    "            #START CV GRIDSEARCH\n",
    "            clf = GridSearchCV(\n",
    "                XGBClassifier(),\n",
    "                parameters,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=12\n",
    "            )\n",
    "            \n",
    "            clf.fit(train_data, train_labels)\n",
    "            print(f'Best parameters: {clf.best_params_}')\n",
    "            auc = roc_auc_score(test_labels, clf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tWith score{auc}')\n",
    "            clf_auc.append(auc)\n",
    "            \n",
    "            #xgb = XGBClassifier()\n",
    "            #xgb.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            #print(f'XGB AUC={auc}')\n",
    "            #xgb_auc.append(auc)\n",
    "            #\n",
    "            #rf = RandomForestClassifier()\n",
    "            #rf.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            #print(f'\\tRF AUC={auc}')\n",
    "            #rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7493606138107417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.724007561436673\n",
    "0.737618147448015\n",
    "0.7552758708365116\n",
    "0.7400255754475704\n",
    "0.7493606138107417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=3, Cut Off=3\n",
      "Fold 0: Best parameters: {'max_depth': 6, 'n_estimators': 10000}\n",
      "\tWith score0.7134215500945179\n",
      "Fold 1: Best parameters: {'max_depth': 6, 'n_estimators': 1000}\n",
      "\tWith score0.7512287334593573\n",
      "Fold 2: Best parameters: {'max_depth': 4, 'n_estimators': 10000}\n",
      "\tWith score0.7396389524535977\n",
      "Fold 3: Best parameters: {'max_depth': 2, 'n_estimators': 100}\n",
      "\tWith score0.7402813299232737\n",
      "Fold 4: Best parameters: {'max_depth': 6, 'n_estimators': 100}\n",
      "\tWith score0.740920716112532\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "\n",
    "clf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [3]:Modes\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            parameters = {\n",
    "                'max_depth': range (2, 10, 2),\n",
    "                'n_estimators': range(40, 220, 40),\n",
    "                'learning_rate': [0.15],\n",
    "                'gamma': [0, 0.1, 0.5, 1],\n",
    "                'lambda': [1, 1.1, 1.5, 2]\n",
    "            }\n",
    "            \n",
    "            parameters = {\n",
    "                'n_estimators': [10, 100, 1000, 10000],\n",
    "                'max_depth': [2, 4, 6],\n",
    "                \n",
    "            }\n",
    "            \n",
    "            #START CV GRIDSEARCH\n",
    "            clf = GridSearchCV(\n",
    "                XGBClassifier(),\n",
    "                parameters,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=12\n",
    "            )\n",
    "            \n",
    "            clf.fit(train_data, train_labels)\n",
    "            print(f'Best parameters: {clf.best_params_}')\n",
    "            auc = roc_auc_score(test_labels, clf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tWith score{auc}')\n",
    "            clf_auc.append(auc)\n",
    "            \n",
    "            #xgb = XGBClassifier()\n",
    "            #xgb.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            #print(f'XGB AUC={auc}')\n",
    "            #xgb_auc.append(auc)\n",
    "            #\n",
    "            #rf = RandomForestClassifier()\n",
    "            #rf.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            #print(f'\\tRF AUC={auc}')\n",
    "            #rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737, [0.726, 0.748]\n"
     ]
    }
   ],
   "source": [
    "get_ci(clf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=3, Cut Off=3\n",
      "Fold 0: /home/grads/g/guangzhou92/enter/envs/py36/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "Best parameters: {'max_depth': 10, 'n_estimators': 10000}\n",
      "\tWith score0.7587901701323253\n",
      "Fold 1: Best parameters: {'max_depth': 6, 'n_estimators': 1000}\n",
      "\tWith score0.7681159420289855\n",
      "Fold 2: Best parameters: {'max_depth': 10, 'n_estimators': 1000}\n",
      "\tWith score0.7962115433511315\n",
      "Fold 3: Best parameters: {'max_depth': 10, 'n_estimators': 5000}\n",
      "\tWith score0.7606138107416879\n",
      "Fold 4: Best parameters: {'max_depth': 10, 'n_estimators': 10000}\n",
      "\tWith score0.758695652173913\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "clf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [3]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            parameters = {\n",
    "                'max_depth': range (2, 10, 2),\n",
    "                'n_estimators': range(40, 220, 40),\n",
    "                'learning_rate': [0.15],\n",
    "                'gamma': [0, 0.1, 0.5, 1],\n",
    "                'lambda': [1, 1.1, 1.5, 2]\n",
    "            }\n",
    "            \n",
    "            parameters = {\n",
    "                'n_estimators': [1000, 5000, 10000],\n",
    "                'max_depth': [6, 8, 10],\n",
    "                \n",
    "            }\n",
    "            \n",
    "            #START CV GRIDSEARCH\n",
    "            clf = GridSearchCV(\n",
    "                RandomForestClassifier(),\n",
    "                parameters,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=12\n",
    "            )\n",
    "            \n",
    "            clf.fit(train_data, train_labels)\n",
    "            print(f'Best parameters: {clf.best_params_}')\n",
    "            auc = roc_auc_score(test_labels, clf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tWith score{auc}')\n",
    "            clf_auc.append(auc)\n",
    "            \n",
    "            #xgb = XGBClassifier()\n",
    "            #xgb.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            #print(f'XGB AUC={auc}')\n",
    "            #xgb_auc.append(auc)\n",
    "            #\n",
    "            #rf = RandomForestClassifier()\n",
    "            #rf.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            #print(f'\\tRF AUC={auc}')\n",
    "            #rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768, [0.756, 0.781]\n"
     ]
    }
   ],
   "source": [
    "get_ci(clf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=3, Cut Off=3\n",
      "Fold 0: /home/grads/g/guangzhou92/enter/envs/py36/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "Best parameters: {'learning_rate': 0.15, 'max_depth': 8, 'n_estimators': 10000}\n",
      "\tWith score0.722621298046629\n",
      "Fold 1: Best parameters: {'learning_rate': 0.15, 'max_depth': 10, 'n_estimators': 10000}\n",
      "\tWith score0.75526149968494\n",
      "Fold 2: Best parameters: {'learning_rate': 0.15, 'max_depth': 8, 'n_estimators': 10000}\n",
      "\tWith score0.7481566234426646\n",
      "Fold 3: Best parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 10000}\n",
      "\tWith score0.7561381074168798\n",
      "Fold 4: Best parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 10000}\n",
      "\tWith score0.7179028132992327\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "\n",
    "clf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [3]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            parameters = {\n",
    "                'max_depth': range (2, 10, 2),\n",
    "                'n_estimators': range(40, 220, 40),\n",
    "                'learning_rate': [0.15],\n",
    "                'gamma': [0, 0.1, 0.5, 1],\n",
    "                'lambda': [1, 1.1, 1.5, 2]\n",
    "            }\n",
    "            \n",
    "            parameters = {\n",
    "                'n_estimators': [1000, 5000, 10000],\n",
    "                'max_depth': [6, 8, 10],\n",
    "                'learning_rate': [0.1, 0.15]\n",
    "                \n",
    "            }\n",
    "            \n",
    "            #START CV GRIDSEARCH\n",
    "            clf = GridSearchCV(\n",
    "                XGBClassifier(),\n",
    "                parameters,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=12\n",
    "            )\n",
    "            \n",
    "            clf.fit(train_data, train_labels)\n",
    "            print(f'Best parameters: {clf.best_params_}')\n",
    "            auc = roc_auc_score(test_labels, clf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tWith score{auc}')\n",
    "            clf_auc.append(auc)\n",
    "            \n",
    "            #xgb = XGBClassifier()\n",
    "            #xgb.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            #print(f'XGB AUC={auc}')\n",
    "            #xgb_auc.append(auc)\n",
    "            #\n",
    "            #rf = RandomForestClassifier()\n",
    "            #rf.fit(train_data, train_labels)\n",
    "            #auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            #print(f'\\tRF AUC={auc}')\n",
    "            #rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740, [0.726, 0.754]\n"
     ]
    }
   ],
   "source": [
    "get_ci(clf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58da084f31c6eb64268c493555b25826b03666fd65fe464a2857c96b7e0e8a8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
