{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/g/guangzhou92/enter/envs/py36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os, datetime\n",
    "import torch\n",
    "import random \n",
    "import numpy as np \n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import cycle\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from pytorch_lightning.metrics.functional.classification import f1_score, auroc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from resnetv2 import PreActResNet18 as ResNet18  \n",
    "from utils import Labeled_dataset\n",
    "from MLP_base import Net as mlp\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from resblock_searched import Appended_Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Cifar10_100 Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='initial learning rate')\n",
    "parser.add_argument('--data_dir', help='The directory for data', default='trans_data', type=str)\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float, help='weight decay')\n",
    "parser.add_argument('--epochs', default=100, type=int, help='number of total epochs to run')\n",
    "parser.add_argument('--print_freq', default=50, type=int, help='print frequency')\n",
    "parser.add_argument('--decreasing_lr', default='60,80', help='decreasing strategy')\n",
    "parser.add_argument('--save_dir', help='The directory used to save the trained models', default='cifar10_cil', type=str)\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
    "parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
    "parser.add_argument('--load_model', default=False, type=eval, choices=[True, False], help='load last checkpoint to continue training')\n",
    "parser.add_argument('--drop_r', default=0.3, type=float, help='drop out rate')\n",
    "parser.add_argument('--out_size', default=10, type=int, help='total possible labels (binary is 1)')\n",
    "parser.add_argument('--lr_Plateau_factor', default=0.1, type=float, help='torch.optim.lr_scheduler.ReduceLROnPlateau: Factor by which the learning rate will be reduced')\n",
    "parser.add_argument('--lr_Plateau_patience', default=10, type=int, help='torch.optim.lr_scheduler.ReduceLROnPlateau: Number of epochs with no improvement after which learning rate will be reduced')\n",
    "\n",
    "\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = [\n",
    "    'death', 'cvd_death', 'time_death', 'anyhosp', 'time_anyhosp',\n",
    "    'hfhosp', 'time_hfhosp', 'abortedca', 'time_abortedca', 'mi',\n",
    "    'time_mi', 'stroke', 'time_stroke', 'primary_ep', 'time_primary_ep'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_cat_cols = [\n",
    "    'GLUCOSE_FAST', 'GLUCOSE_RAND', 'CO2_mmolL', 'GLUCOSE_mgdL','WBC_kuL',\n",
    "    'HCT_p', 'HB_gdL', 'PLT_kuL', 'ALP_UL', 'TBILI_mgdL', 'ALB_gdL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_cols = [\n",
    "    'BNP_VAL', 'age_entry', 'EF', 'visit_dt1_hf', 'chfdc_dt3', 'mi_dt3',\n",
    "    'stroke_dt3', 'cabg_dt3', 'pci_dt3', 'DM_AGE_YR', 'DM_DUR_YR', 'cigs',\n",
    "    'SMOKE_YRS', 'QUIT_YRS', 'HEAVY_MIN', 'HEAVY_WK', 'MED_WK', 'MED_MIN',\n",
    "    'LIGHT_WK', 'LIGHT_MIN', 'metsperweek', 'cooking_salt_score', 'height',\n",
    "    'weight', 'waistc', 'HR', 'SBP', 'DBP', 'CR_mgdl', 'gfr', 'labs_dt1',\n",
    "    'NA_mmolL', 'K_mmolL', 'CL_mmolL', 'BUN_mgdL', 'ALT_UL', 'AST_UL',\n",
    "    'urine_val_mgg', 'QRS_DUR', 'CR_mgdL', 'BMI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "priep = pd.read_csv('/data/datasets/topcat/nch/nn_baseline/primary_ep_set.csv', index_col=0)\n",
    "death = pd.read_csv('/data/datasets/topcat/nch/nn_baseline/death_set.csv'     , index_col=0)\n",
    "hfhos = pd.read_csv('/data/datasets/topcat/nch/nn_baseline/hfhosp_set.csv'    , index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 2\n",
    "\n",
    "if mode == 1:\n",
    "    outcome = 'primary_ep'\n",
    "    outcome_time = 'time_primary_ep'\n",
    "    df = priep.copy()\n",
    "elif mode == 2:\n",
    "    outcome = 'death'\n",
    "    outcome_time = 'time_death'\n",
    "    df = death.copy()\n",
    "elif mode == 3:\n",
    "    outcome = 'hfhosp'\n",
    "    outcome_time = 'time_hfhosp'\n",
    "    df = hfhos.copy()\n",
    "\n",
    "labels = df[outcome].copy()\n",
    "complete_labels = labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, if_main=False):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    aucrocs = AverageMeter()\n",
    "    aucprs = AverageMeter()\n",
    "    f1_scores = AverageMeter()\n",
    "    \n",
    "    # for confusion matrix\n",
    "    #pred_ls = torch.empty(len(val_loader.dataset))\n",
    "    #true_ls = torch.empty(len(val_loader.dataset))\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        #true_ls[val_loader.batch_size * i, val_loader.batch_size * (i+1)] = target\n",
    "        \n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.long().cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            \n",
    "            if args.out_size == 1:\n",
    "                output = torch.sigmoid(output)\n",
    "            \n",
    "            #loss = criterion(output, target)\n",
    "            loss = criterion_ExpandTarget(output, target, criterion)\n",
    "            \n",
    "            \n",
    "        output = output.float()\n",
    "        #pred_ls[val_loader.batch_size * i, val_loader.batch_size * (i+1)] = np.argmax(output, axis=1)\n",
    "        \n",
    "        loss = loss.float()    \n",
    "        \n",
    "          \n",
    "        aurocsore = auroc(output[:, 1].data, target)  \n",
    "        #aurocsore = roc_auc_score(target, output[:, 1])\n",
    "        #aucprscore = aucpr(output.data, target)  \n",
    "        #f1score = f1_score(output.data, target, pos_label=1)\n",
    "        \n",
    "        \n",
    "        aucrocs.update(aurocsore.item(), input.size(0))\n",
    "        #aucprs.update(aucprscore.item(), input.size(0))\n",
    "        #f1_scores.update(f1score.item(), input.size(0))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "\n",
    "        '''   \n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]  '\n",
    "                  'Loss {loss.avg:.4f}  '\n",
    "                  'aucroc {aucrocs.avg:.3f}  '\n",
    "                  #'aucpr {aucprs.avg:.3f}  '\n",
    "                  #'f1 {f1_scores.avg:.3f}'\n",
    "                  .format(i, len(val_loader), loss=losses, \n",
    "                          aucrocs=aucrocs, \n",
    "                          #aucprs=aucprs, \n",
    "                          #f1_scores=f1_scores\n",
    "                         ))\n",
    "          \n",
    "    \n",
    "    print('aucroc {aucrocs.avg:.3f}  '\n",
    "          #'aucpr {aucprs.avg:.3f}  '\n",
    "          #'f1 {f1_scores.avg:.3f}'\n",
    "          .format(aucrocs=aucrocs, \n",
    "                  #aucprs=aucprs, f1_scores=f1_scores\n",
    "                 ))'''\n",
    "    \n",
    "    \n",
    "    #print_confusion_matrix(pred_ls, true_ls, labels=[0, 1])\n",
    "    \n",
    "    #return statistics.mean(f1_score_ls), statistics.mean(accuracy_ls), statistics.mean(auroc_score_ls), statistics.mean(auc_pr_ls)\n",
    "    return losses.avg, aucrocs.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#baseline training: not decoupling\n",
    "\n",
    "def training(rand_loader, new_balance_loader, old_balance_loader, model, criterion, optimizer, epoch):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    coef_old = int(args.batch_size/2)/64\n",
    "    coef_new = int(args.batch_size/2)/64\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    new_balance = iter(new_balance_loader)\n",
    "    old_balance = iter(old_balance_loader)\n",
    "\n",
    "    for i, (input, target) in enumerate(rand_loader):\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            bal_new_img, bal_new_target = next(new_balance)\n",
    "        except StopIteration:\n",
    "            new_balance = iter(new_balance_loader)\n",
    "            bal_new_img, bal_new_target = next(new_balance)\n",
    "\n",
    "        try:\n",
    "            bal_old_img, bal_old_target = next(old_balance)\n",
    "        except StopIteration:\n",
    "            old_balance = iter(old_balance_loader)\n",
    "            bal_old_img, bal_old_target = next(old_balance)\n",
    "        \n",
    "\n",
    "        bal_new_img = bal_new_img.cuda()\n",
    "        bal_old_img = bal_old_img.cuda()\n",
    "        '''\n",
    "        input = input.cuda()\n",
    "\n",
    "        '''\n",
    "        bal_new_target = bal_new_target.long().cuda()\n",
    "        bal_old_target = bal_old_target.long().cuda()\n",
    "        '''\n",
    "        target = target.long().cuda()\n",
    "\n",
    "        # random input\n",
    "        output_gt = model(input, main_fc=False)\n",
    "        loss_rand = criterion(output_gt, target)\n",
    "        \n",
    "        '''\n",
    "        # balance inputs\n",
    "        output_bal_new = model(bal_new_img, main_fc=True)\n",
    "        output_bal_old = model(bal_old_img, main_fc=True)\n",
    "        loss_balance = criterion(output_bal_new, bal_new_target)*coef_new + criterion(output_bal_old, bal_old_target)*coef_old\n",
    "        '''\n",
    "        \n",
    "        if tensor_allNaN(output_gt):\n",
    "            print('output_gt')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(loss_rand):\n",
    "            print('loss_rand')\n",
    "            sys.exit()\n",
    "        '''\n",
    "        if tensor_allNaN(output_bal_new):\n",
    "            print('output_bal_new')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(output_bal_old):\n",
    "            print('output_bal_old')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(loss_balance):\n",
    "            print('loss_balance')  \n",
    "            sys.exit()\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #loss = (loss_balance + loss_rand)*0.5\n",
    "        loss = loss_rand\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output_gt.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = auroc(output.data, target)   \n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'auroc {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(rand_loader), loss=losses, top1=top1))\n",
    "            \n",
    "\n",
    "    print('train_accuracy {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(rand_loader, new_balance_loader, old_balance_loader, model, criterion, optimizer_BB, optimizer_CB, optimizer_R, epoch):\n",
    "    \n",
    "    losses_rand = AverageMeter()\n",
    "    top1_rand = AverageMeter()\n",
    "    \n",
    "    losses_bal = AverageMeter()\n",
    "    top1_bal = AverageMeter()\n",
    "    \n",
    "\n",
    "    coef_old = 0.5\n",
    "    coef_new = 0.5\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    new_balance = iter(new_balance_loader)\n",
    "    old_balance = iter(old_balance_loader)\n",
    "\n",
    "    for i, (input, target) in enumerate(rand_loader):\n",
    "\n",
    "        \n",
    "        try:\n",
    "            bal_new_img, bal_new_target = next(new_balance)\n",
    "        except StopIteration:\n",
    "            new_balance = iter(new_balance_loader)\n",
    "            bal_new_img, bal_new_target = next(new_balance)\n",
    "\n",
    "        try:\n",
    "            bal_old_img, bal_old_target = next(old_balance)\n",
    "        except StopIteration:\n",
    "            old_balance = iter(old_balance_loader)\n",
    "            bal_old_img, bal_old_target = next(old_balance)\n",
    "        \n",
    "\n",
    "        bal_new_img = bal_new_img.cuda()\n",
    "        bal_old_img = bal_old_img.cuda()\n",
    "        \n",
    "        input = input.cuda()\n",
    "\n",
    "        \n",
    "        bal_new_target = bal_new_target.long().cuda()\n",
    "        bal_old_target = bal_old_target.long().cuda()\n",
    "        \n",
    "        target = target.long().cuda()\n",
    "\n",
    "        # random input\n",
    "        output_gt = model(input, main_fc=False)\n",
    "        #loss_rand = criterion(output_gt, target)\n",
    "        loss_rand = criterion_ExpandTarget(output_gt, target, criterion)\n",
    "\n",
    "        \n",
    "        # balance inputs\n",
    "        output_bal_new = model(bal_new_img, main_fc=True)\n",
    "        output_bal_old = model(bal_old_img, main_fc=True)\n",
    "        \n",
    "        \n",
    "        #loss_new = criterion(output_bal_new, bal_new_target.unsqueeze(1).type_as(output_bal_new))*coef_new\n",
    "        loss_new = criterion_ExpandTarget(output_bal_new, bal_new_target, criterion)*coef_new\n",
    "        \n",
    "        \n",
    "        #loss_old = criterion(output_bal_old, bal_old_target.unsqueeze(1).type_as(output_bal_old))*coef_old\n",
    "        loss_old = criterion_ExpandTarget(output_bal_old, bal_old_target, criterion)*coef_old\n",
    "        \n",
    "        loss_balance = loss_new + loss_old\n",
    "        \n",
    "        # check if any output is NaN\n",
    "        if tensor_allNaN(output_gt):\n",
    "            print('output_gt')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(loss_rand):\n",
    "            print('loss_rand')\n",
    "            sys.exit()\n",
    "        \n",
    "        if tensor_allNaN(output_bal_new):\n",
    "            print('output_bal_new')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(output_bal_old):\n",
    "            print('output_bal_old')\n",
    "            sys.exit()\n",
    "        if tensor_allNaN(loss_balance):\n",
    "            print('loss_balance')  \n",
    "            sys.exit()\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = (loss_balance + loss_rand)*0.5\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        optimizer_BB.zero_grad()\n",
    "        optimizer_CB.zero_grad()\n",
    "        optimizer_R.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer.step()\n",
    "        optimizer_BB.step()\n",
    "        optimizer_CB.step()\n",
    "        optimizer_R.step()\n",
    "\n",
    "        output = output_gt.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss, for balanced classifier\n",
    "        output_cpu = output.cpu().detach().numpy()\n",
    "        output_bal_new_cpu = output_bal_new.cpu().detach().numpy()\n",
    "        output_bal_old_cpu = output_bal_old.cpu().detach().numpy()\n",
    "        output_bal_cpu = np.concatenate((output_bal_new_cpu, output_bal_old_cpu), axis=0)\n",
    "        \n",
    "        target_cpu = target.cpu().detach().numpy()\n",
    "        bal_new_target_cpu = bal_new_target.cpu().detach().numpy()\n",
    "        bal_old_target_cpu = bal_old_target.cpu().detach().numpy()\n",
    "        bal_target_cpu = np.concatenate((bal_new_target_cpu, bal_old_target_cpu), axis=0)\n",
    "        \n",
    "        \n",
    "        #prec1 = auroc(output.data, target)   \n",
    "        auroc_rand = roc_auc_score(target_cpu, output_cpu[:, 1])\n",
    "        auroc_bal = roc_auc_score(bal_target_cpu, output_bal_cpu[:, 1])\n",
    "        \n",
    "        \n",
    "        losses_rand.update(loss_rand.item(), input.size(0))        \n",
    "        losses_bal.update(loss_balance.item(), output_bal_cpu.shape[0])\n",
    "        top1_rand.update(auroc_rand.item(), input.size(0))\n",
    "        top1_bal.update(auroc_bal.item(), output_bal_cpu.shape[0])\n",
    "\n",
    "        '''\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'auroc {top1_rand.val:.3f} ({top1_rand.avg:.3f})'.format(\n",
    "                      epoch, i, len(rand_loader), loss=losses_rand, top1_rand=top1_rand))\n",
    "            \n",
    "\n",
    "    print('train_accuracy {top1_rand.avg:.3f}'.format(top1_rand=top1_rand))'''\n",
    "\n",
    "    return losses_rand.avg, losses_bal.avg, top1_bal.avg, top1_rand.avg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[0.1, 0.9], [0.8, 0.2], [0.7, 0.3]])\n",
    "# b = torch.tensor([1, 1, 0])\n",
    "# aucpr(a, b)\n",
    "# tensor(0.8333, dtype=torch.float64)\n",
    "\n",
    "def aucpr(pred, true, average='macro'):\n",
    "    pred = pred.numpy()[:, 1]\n",
    "    true = true.numpy()\n",
    "    aucprscore = average_precision_score(true, pred, average=average)\n",
    "    aucprscore = torch.tensor(aucprscore)\n",
    "    \n",
    "    return aucprscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(pred, true, labels=[0, 1]):\n",
    "    pred = pred.numpy()\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "    y_true = true.numpy()\n",
    "    \n",
    "    unique_label = np.unique([y_true, y_pred])\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_true, y_pred, labels=unique_label), \n",
    "        index=['true:{:}'.format(x) for x in unique_label], \n",
    "        columns=['pred:{:}'.format(x) for x in unique_label]\n",
    "    )\n",
    "    print(cmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_allNaN(tensor):\n",
    "    all_Nan = torch.isnan(tensor).all()\n",
    "    if all_Nan:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='weight.pt'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class define_NN_model(object):\n",
    "    def __init__(self, h_sizes, drop_r, out_size):\n",
    "        self.h_sizes = h_sizes\n",
    "        self.drop_r = drop_r\n",
    "        self.out_size = out_size\n",
    "    \n",
    "    def def_model(self):\n",
    "        model = mlp(self.h_sizes, self.drop_r, self.out_size)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), args.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decreasing_lr, gamma=0.1)\n",
    "        \n",
    "        return model, optimizer, scheduler\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand target labels to one hot encoding\n",
    "\n",
    "# Example:\n",
    "#target = torch.ones([10, 1], dtype=torch.float32)  # 64 classes, batch size = 10\n",
    "#output = torch.full([10, 2], 1.5)  # A prediction (logit)\n",
    "#pos_weight = torch.ones([2])  # All weights are equal to 1\n",
    "#criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "def criterion_ExpandTarget(output, target, criterion):\n",
    "\n",
    "    nb_classes = 2\n",
    "    batch_size = output.size()[0]\n",
    "    \n",
    "    #target_one_hot = torch.nn.functional.one_hot(target.to(torch.int64))\n",
    "    \n",
    "    target_one_hot = torch.FloatTensor(batch_size, nb_classes)\n",
    "    target_one_hot.zero_()\n",
    "    target_one_hot = target_one_hot.cuda()\n",
    "    \n",
    "    target = target.view(-1,1)\n",
    "    \n",
    "    target_one_hot.scatter_(1, target, 1)\n",
    "    \n",
    "    loss = criterion(output, target_one_hot.type_as(output)) \n",
    "    #loss = criterion(output, target)\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== below ================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args, best_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=440, data_dir='trans_data', decreasing_lr='60,80', drop_r=0.3, epochs=20, gpu=3, load_model=False, lr=0.058711765521739734, lr_Plateau_factor=0.4937429181024774, lr_Plateau_patience=8, momentum=0.11292347293920574, out_size=2, print_freq=200, save_dir='output', seed=1, weight_decay=0.014324063825534989)\n"
     ]
    }
   ],
   "source": [
    "# jupyter notebook input workaround\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=['--save_dir', 'output', \n",
    "                               '--data_dir', 'trans_data', \n",
    "                               '--gpu', '3', \n",
    "                               '--epochs', '20', \n",
    "                               '--load_model', 'False',\n",
    "                               '--seed', '1',\n",
    "                               '--lr', '0.058711765521739734',\n",
    "                               '--print_freq', '200',\n",
    "                               '--out_size', '2',\n",
    "                               '--batch_size', '440',\n",
    "                               '--weight_decay', '0.014324063825534989',\n",
    "                               '--momentum', '0.11292347293920574',\n",
    "                               '--lr_Plateau_factor', '0.4937429181024774',\n",
    "                               '--lr_Plateau_patience', '8',                               \n",
    "                              ])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.out_size == 2:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "decreasing_lr = list(map(int, args.decreasing_lr.split(',')))\n",
    "\n",
    "\n",
    "\n",
    "# model = mlp(h_sizes, args.drop_r, args.out_size)\n",
    "# model.cuda()\n",
    "\n",
    "starting_epoch = 0\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), args.lr)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decreasing_lr, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 02:40:15 PM  XGB AUC=0.705\tNNs:\n",
      "RF AUC=0.735\n",
      "NNs:\n",
      "AUC= 0.7470395565032959\n",
      "Fold 1: 02:40:44 PM  XGB AUC=0.737\tNNs:\n",
      "RF AUC=0.746\n",
      "NNs:\n",
      "AUC= 0.791509211063385\n",
      "Fold 2: 02:41:15 PM  XGB AUC=0.731\tNNs:\n",
      "RF AUC=0.756\n",
      "NNs:\n",
      "AUC= 0.791509211063385\n",
      "Fold 3: 02:41:46 PM  XGB AUC=0.688\tNNs:\n",
      "RF AUC=0.719\n",
      "NNs:\n",
      "AUC= 0.791509211063385\n",
      "Fold 4: 02:42:18 PM  XGB AUC=0.676\tNNs:\n",
      "RF AUC=0.666\n",
      "NNs:\n",
      "AUC= 0.791509211063385\n",
      "            0         1         2         3         4  lower_ci      mean  \\\n",
      "xgb  0.704557  0.736918  0.731493  0.688345  0.676275  0.697162  0.707518   \n",
      "rf   0.735279  0.746237  0.755738  0.719303  0.665609  0.710491  0.724433   \n",
      "res  0.747040  0.791509  0.791509  0.791509  0.791509  0.774819  0.782615   \n",
      "\n",
      "     upper_ci  \n",
      "xgb  0.717873  \n",
      "rf   0.738375  \n",
      "res  0.790411  \n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "auc_df = pd.DataFrame(index=range(5))\n",
    "\n",
    "for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "    best_prec1_fold = 0\n",
    "\n",
    "    ########## prepare data ##################\n",
    "    print(f'Fold {i}: {datetime.datetime.now().strftime(\"%I:%M:%S %p\")}  ', end='')\n",
    "    train_data = df.iloc[train].copy()\n",
    "    test_data = df.iloc[test].copy()\n",
    "\n",
    "    train_labels=labels.iloc[train].copy()\n",
    "    test_labels=labels.iloc[test].copy()\n",
    "\n",
    "    weights = len(train_labels)/test_labels.sum()\n",
    "    glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "    glm_weights.loc[train_labels==1] = weights\n",
    "\n",
    "    ## preprocessing: remove ID/label/std=0 columns, mean imputation, normalization\n",
    "    train_id = train_data['ID'].copy()\n",
    "    test_id = test_data['ID'].copy()\n",
    "\n",
    "    train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "    test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "\n",
    "    #print(f'Fold {i} Imputation')\n",
    "    #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #train_data.values = imp.fit_transform(train_data)\n",
    "    #test_data.values  = imp.transform(test_data)\n",
    "    test_data = test_data.fillna(train_data.mean())\n",
    "    train_data = train_data.fillna(train_data.mean())\n",
    "\n",
    "    sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "    train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "    test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "\n",
    "    cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "    scaler = StandardScaler()\n",
    "    train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "    test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "\n",
    "    ############# prepare data for balanced batch ###############\n",
    "    pos_idx = np.where(train_labels == 1)[0]\n",
    "    neg_idx = np.where(train_labels == 0)[0]\n",
    "\n",
    "    train_data_pos = torch.Tensor(train_data.iloc[pos_idx].values)\n",
    "    train_data_neg = torch.Tensor(train_data.iloc[neg_idx].values)\n",
    "    train_data_t = torch.Tensor(train_data.values)\n",
    "\n",
    "    train_label_pos = torch.Tensor(train_labels.iloc[pos_idx].values)\n",
    "    train_label_neg = torch.Tensor(train_labels.iloc[neg_idx].values)\n",
    "    train_labels_t = torch.Tensor(train_labels.values)\n",
    "\n",
    "    train_Dataset_pos = TensorDataset(train_data_pos, train_label_pos)\n",
    "    train_Dataset_neg = TensorDataset(train_data_neg, train_label_neg)\n",
    "    train_Dataset = TensorDataset(train_data_t, train_labels_t)\n",
    "\n",
    "    train_loader_pos = DataLoader(train_Dataset_pos, batch_size=int(args.batch_size/2), shuffle=True, num_workers=2, pin_memory=True)\n",
    "    train_loader_neg = DataLoader(train_Dataset_neg, batch_size=int(args.batch_size/2), shuffle=True, num_workers=2, pin_memory=True)\n",
    "    train_loader = DataLoader(train_Dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    ############# prepare data for balanced batch (validation) ###############\n",
    "    pos_idx = np.where(test_labels == 1)[0]\n",
    "    neg_idx = np.where(test_labels == 0)[0]\n",
    "    neg_idx = np.random.choice(neg_idx, len(pos_idx))\n",
    "    \n",
    "    test_data_pos = torch.Tensor(test_data.iloc[pos_idx].values)\n",
    "    test_data_neg = torch.Tensor(test_data.iloc[neg_idx].values)\n",
    "    test_data_bal = torch.cat((test_data_pos, test_data_neg), 0)\n",
    "    test_labels_pos = torch.Tensor(test_labels.iloc[pos_idx].values)\n",
    "    test_labels_neg = torch.Tensor(test_labels.iloc[neg_idx].values)\n",
    "    test_labels_bal = torch.cat((test_labels_pos, test_labels_neg), 0)\n",
    "    valid_Dataset_bal = TensorDataset(test_data_bal, test_labels_bal)\n",
    "    val_loader_bal = DataLoader(valid_Dataset_bal, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "\n",
    "    valid_data_t = torch.Tensor(test_data.values)\n",
    "    test_labels_t = torch.Tensor(test_labels.values)\n",
    "    valid_Dataset = TensorDataset(valid_data_t, test_labels_t)\n",
    "    val_loader = DataLoader(valid_Dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "    ############ XGB ################\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(train_data, train_labels)\n",
    "    auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "    print(f'XGB AUC={auc:.3f}\\tNNs:')\n",
    "    if i == 0:\n",
    "        auc_df['xgb'] = np.nan\n",
    "    auc_df['xgb'].iloc[i] = auc\n",
    "\n",
    "    ############ RF ################\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(train_data, train_labels)\n",
    "    auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "    print(f'RF AUC={auc:.3f}\\nNNs:')\n",
    "    if i == 0:\n",
    "        auc_df['rf'] = np.nan\n",
    "    auc_df['rf'].iloc[i] = auc\n",
    "\n",
    "\n",
    "    ############ ResBlock ################\n",
    "    '''\n",
    "    model = torch.load('Auto-PyTorch/examples/basics/TOPCAT_search.pt')\n",
    "    model.cuda()\n",
    "    auroc_meter = AverageMeter()\n",
    "    for data, target in val_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.long().cuda()\n",
    "        output = model(data)\n",
    "        output = torch.sigmoid(output)\n",
    "        aurocsore = auroc(output.data[:,1], target) \n",
    "        auroc_meter.update(aurocsore, target.size(0))\n",
    "    print(auroc_meter.avg.cpu().detach().numpy())\n",
    "    if i == 0:\n",
    "        auc_df['res'] = np.nan\n",
    "    auc_df['res'].iloc[i] = auroc_meter.avg.cpu().detach().numpy()\n",
    "    '''\n",
    "\n",
    "    model = Appended_Model(last_in=79, last_out=2)\n",
    "    model.cuda()\n",
    "    optimizer_BB = torch.optim.SGD(model.backbone.parameters(), args.lr * 10e-8, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler_BB = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_BB, factor=args.lr_Plateau_factor, patience=args.lr_Plateau_patience)\n",
    "    \n",
    "    optimizer_CB = torch.optim.SGD(model.linear_main.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler_CB = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_CB, factor=args.lr_Plateau_factor, patience=args.lr_Plateau_patience)\n",
    "    \n",
    "    optimizer_R = torch.optim.SGD(model.linear.parameters(), args.lr* 10e-5, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler_R = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_R, factor=args.lr_Plateau_factor, patience=args.lr_Plateau_patience)\n",
    "    \n",
    "    \n",
    "\n",
    "    # [index, acc]\n",
    "    train_acc_bal = [[],[]]\n",
    "    train_acc_rand = [[],[]]\n",
    "    ta_bal = [[],[]]\n",
    "    ta_imba = [[],[]]\n",
    "\n",
    "    for epoch in range(starting_epoch, starting_epoch + args.epochs):\n",
    "        loss_rand_train, loss_bal_train, top1_bal_train, top1_rand_train = training(train_loader, train_loader_pos, train_loader_neg, model, criterion, optimizer_BB, optimizer_CB, optimizer_R, epoch)\n",
    "\n",
    "\n",
    "        loss_rand_test, prec1_bal_test = validate(val_loader_bal, model, criterion, if_main=True)\n",
    "        loss_bal_test, prec1_imba_test = validate(val_loader_bal, model, criterion, if_main=False)\n",
    "\n",
    "        train_acc_bal[0].append(epoch)\n",
    "        train_acc_rand[0].append(epoch)\n",
    "        ta_bal[0].append(epoch)\n",
    "        ta_imba[0].append(epoch)\n",
    "\n",
    "        train_acc_bal[1].append(top1_bal_train)\n",
    "        train_acc_rand[1].append(top1_rand_train)\n",
    "        ta_bal[1].append(prec1_bal_test)\n",
    "        ta_imba[1].append(prec1_imba_test)\n",
    "\n",
    "        #scheduler.step(metrics=prec1_bal_test)\n",
    "        scheduler_BB.step(metrics=prec1_bal_test)\n",
    "        scheduler_CB.step(metrics=prec1_bal_test)\n",
    "        scheduler_R.step(metrics=prec1_bal_test)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        # balanced, testing result\n",
    "        if prec1_bal_test > best_prec1:\n",
    "            is_best = prec1_bal_test > best_prec1\n",
    "            best_prec1 = max(prec1_bal_test, best_prec1)\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if is_best:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                'optimizer_BB': optimizer_BB.state_dict(),\n",
    "                'optimizer_CB': optimizer_CB.state_dict(),\n",
    "                'optimizer_R': optimizer_R.state_dict(),\n",
    "                'scheduler_BB': scheduler_BB.state_dict(),\n",
    "                'scheduler_CB': scheduler_CB.state_dict(),\n",
    "                'scheduler_R': scheduler_R.state_dict(),\n",
    "            }, filename=os.path.join(args.save_dir, 'best_model.pt'))\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer_BB': optimizer_BB.state_dict(),\n",
    "            'optimizer_CB': optimizer_CB.state_dict(),\n",
    "            'optimizer_R': optimizer_R.state_dict(),\n",
    "            'scheduler_BB': scheduler_BB.state_dict(),\n",
    "            'scheduler_CB': scheduler_CB.state_dict(),\n",
    "            'scheduler_R': scheduler_R.state_dict(),\n",
    "        }, filename=os.path.join(args.save_dir, 'checkpoint.pt'))\n",
    "\n",
    "        plt.plot(train_acc_bal[0], train_acc_bal[1], label='train_acc_bal')\n",
    "        plt.plot(train_acc_rand[0], train_acc_rand[1], label='train_acc_rand')\n",
    "        plt.plot(ta_imba[0], ta_imba[1], label='TA_imba')\n",
    "        plt.plot(ta_bal[0], ta_bal[1], label='TA_bal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(args.save_dir, 'net_train.png'))\n",
    "        if is_best:\n",
    "            plt.savefig(os.path.join(args.save_dir, 'net_train_best_epoch'+ str(epoch) + '.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # for dataframe\n",
    "        auc = best_prec1\n",
    "        if best_prec1 < best_prec1_fold:\n",
    "            auc = best_prec1_fold\n",
    "        else:\n",
    "            best_prec1_fold = best_prec1\n",
    "\n",
    "\n",
    "    '''\n",
    "    # quick validation (when without validation function)\n",
    "    auroc_meter = AverageMeter()\n",
    "    model.eval()\n",
    "    for data, target in val_loader:\n",
    "        data = data.cuda()\n",
    "        target = target.long().cuda()\n",
    "        output = model(data)\n",
    "        output = torch.sigmoid(output)\n",
    "        aurocsore = auroc(output.data[:,1], target) \n",
    "        auroc_meter.update(aurocsore, target.size(0))\n",
    "    print(auroc_meter.avg.cpu().detach().numpy())\n",
    "    '''\n",
    "\n",
    "    if i == 0:\n",
    "        auc_df['res'] = np.nan\n",
    "\n",
    "    '''\n",
    "    # only quick test\n",
    "    auc_df['res'].iloc[i] = auroc_meter.avg.cpu().detach().numpy()\n",
    "    '''\n",
    "\n",
    "    # real training\n",
    "    auc_df['res'].iloc[i] = best_prec1_fold\n",
    "    print('AUC=', best_prec1_fold)\n",
    "\n",
    "\n",
    "auc_scores = auc_df.T\n",
    "auc_scores['lower_ci'] = auc_df.T.mean(axis=1) - 1.96*auc_df.T.std(axis=1)/auc_df.T.count(axis=1)\n",
    "auc_scores['mean'] = auc_df.T.mean(axis=1)\n",
    "auc_scores['upper_ci'] = auc_df.T.mean(axis=1) + 1.96*auc_df.T.std(axis=1)/auc_df.T.count(axis=1)\n",
    "#auc_scores.sort_values(by='mean', ascending=False)\n",
    "\n",
    "print(auc_scores)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f5cc6975a40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.parameters()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.704557</td>\n",
       "      <td>0.735279</td>\n",
       "      <td>0.747040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.736918</td>\n",
       "      <td>0.746237</td>\n",
       "      <td>0.791509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.731493</td>\n",
       "      <td>0.755738</td>\n",
       "      <td>0.791509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688345</td>\n",
       "      <td>0.719303</td>\n",
       "      <td>0.791509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.676275</td>\n",
       "      <td>0.665609</td>\n",
       "      <td>0.791509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        xgb        rf       res\n",
       "0  0.704557  0.735279  0.747040\n",
       "1  0.736918  0.746237  0.791509\n",
       "2  0.731493  0.755738  0.791509\n",
       "3  0.688345  0.719303  0.791509\n",
       "4  0.676275  0.665609  0.791509"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>lower_ci</th>\n",
       "      <th>mean</th>\n",
       "      <th>upper_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xgb</td>\n",
       "      <td>0.704557</td>\n",
       "      <td>0.736918</td>\n",
       "      <td>0.731493</td>\n",
       "      <td>0.688345</td>\n",
       "      <td>0.676275</td>\n",
       "      <td>0.697162</td>\n",
       "      <td>0.707518</td>\n",
       "      <td>0.717873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf</td>\n",
       "      <td>0.735279</td>\n",
       "      <td>0.746237</td>\n",
       "      <td>0.755738</td>\n",
       "      <td>0.719303</td>\n",
       "      <td>0.665609</td>\n",
       "      <td>0.710491</td>\n",
       "      <td>0.724433</td>\n",
       "      <td>0.738375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>res</td>\n",
       "      <td>0.747040</td>\n",
       "      <td>0.791509</td>\n",
       "      <td>0.791509</td>\n",
       "      <td>0.791509</td>\n",
       "      <td>0.791509</td>\n",
       "      <td>0.774819</td>\n",
       "      <td>0.782615</td>\n",
       "      <td>0.790411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4  lower_ci      mean  \\\n",
       "xgb  0.704557  0.736918  0.731493  0.688345  0.676275  0.697162  0.707518   \n",
       "rf   0.735279  0.746237  0.755738  0.719303  0.665609  0.710491  0.724433   \n",
       "res  0.747040  0.791509  0.791509  0.791509  0.791509  0.774819  0.782615   \n",
       "\n",
       "     upper_ci  \n",
       "xgb  0.717873  \n",
       "rf   0.738375  \n",
       "res  0.790411  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Nate's search (delete)\n",
    "'''\n",
    "    ########### MLP #################\n",
    "    for hl_count in [1, 2, 3, 4]:\n",
    "        print(f'\\t{hl_count} HL(s): ', end='')\n",
    "        for hl_size in np.linspace(30,150,dtype=int, num=6):\n",
    "            best_prec1 = 0\n",
    "            \n",
    "            h_sizes = [79] + [hl_size for x in range(hl_count)]\n",
    "            \n",
    "            #define model, optimizer, scheduler\n",
    "            NN_model = define_NN_model(h_sizes, args.drop_r, args.out_size)\n",
    "            model, optimizer, scheduler = NN_model.def_model()\n",
    "            #model = torch.load('Auto-PyTorch/examples/basics/TOPCAT_search.pt')\n",
    "            model.cuda()\n",
    "\n",
    "            # index, acc\n",
    "            train_acc = [[],[]]\n",
    "            ta_bal = [[],[]]\n",
    "            ta_imba = [[],[]]\n",
    "\n",
    "            for epoch in range(starting_epoch, starting_epoch + args.epochs):\n",
    "                train_accuracy = training(train_loader, train_loader_pos, train_loader_neg, model, criterion, optimizer, epoch)\n",
    "\n",
    "                prec1_bal = validate(val_loader, model, criterion, if_main=True)\n",
    "                prec1_imba = validate(val_loader, model, criterion, if_main=False)\n",
    "\n",
    "                train_acc[0].append(epoch)\n",
    "                ta_bal[0].append(epoch)\n",
    "                ta_imba[0].append(epoch)\n",
    "\n",
    "                train_acc[1].append(train_accuracy)\n",
    "                ta_bal[1].append(prec1_bal)\n",
    "                ta_imba[1].append(prec1_imba)\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                # remember best prec@1 and save checkpoint\n",
    "                if prec1_bal > best_prec1:\n",
    "                    is_best = prec1_bal > best_prec1\n",
    "                    best_prec1 = max(prec1_bal, best_prec1)\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                if is_best:\n",
    "                    save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'best_prec1': best_prec1,\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                    }, filename=os.path.join(args.save_dir, 'best_model.pt'))\n",
    "\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_prec1': best_prec1,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                }, filename=os.path.join(args.save_dir, 'checkpoint.pt'))\n",
    "\n",
    "                plt.plot(train_acc[0], train_acc[1], label='train_acc')\n",
    "                plt.plot(ta_imba[0], ta_imba[1], label='TA_imba')\n",
    "                plt.plot(ta_bal[0], ta_bal[1], label='TA_bal')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.savefig(os.path.join(args.save_dir, 'net_train.png'))\n",
    "                if is_best:\n",
    "                    plt.savefig(os.path.join(args.save_dir, 'net_train_'+ f'{hl_count}_{hl_size}' +'.png'))\n",
    "                plt.close()\n",
    "                \n",
    "\n",
    "            \n",
    "            auc = best_prec1\n",
    "            print(f'size{hl_size}=> AUC={auc:.3f} ({best_epoch} epochs) ', end='')\n",
    "            if best_prec1 < best_prec1_fold:\n",
    "                auc = best_prec1_fold\n",
    "            else:\n",
    "                best_prec1_fold = best_prec1\n",
    "            \n",
    "            \n",
    "        print()\n",
    "        \n",
    "    if i == 0:\n",
    "        auc_df['mlp'] = np.nan\n",
    "    auc_df['mlp'].iloc[i] = auc\n",
    "        \n",
    "    #to see in one fold first \n",
    "    break    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58da084f31c6eb64268c493555b25826b03666fd65fe464a2857c96b7e0e8a8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
