{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3abff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b369fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a262706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea31ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32be3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci(X):\n",
    "    mean = np.mean(X)\n",
    "    bound = 1.96*np.std(X)/np.sqrt(len(X))\n",
    "    print(f'{mean:.3f}, [{mean-bound:.3f}, {mean+bound:.3f}]')\n",
    "    #return mean, mean-bound, mean+bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4a3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = [\n",
    "    'death',\n",
    "    'cvd_death',\n",
    "    'time_death',\n",
    "    'anyhosp',\n",
    "    'time_anyhosp',\n",
    "    'hfhosp',\n",
    "    'time_hfhosp',\n",
    "    'abortedca',\n",
    "    'time_abortedca',\n",
    "    'mi',\n",
    "    'time_mi',\n",
    "    'stroke',\n",
    "    'time_stroke',\n",
    "    'primary_ep',\n",
    "    'time_primary_ep'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b785c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_drops = [\n",
    "     'mr_mod',\n",
    "     'symp_bur_score',\n",
    "     'tot_symp_score',\n",
    "     'self_eff_score',\n",
    "     'qol_score',\n",
    "     'soc_limit_score',\n",
    "     'overall_sum_score',\n",
    "     'clin_sum_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435e77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_cat_cols = [\n",
    "    'GLUCOSE_FAST',\n",
    "    'GLUCOSE_RAND',\n",
    "    'CO2_mmolL',\n",
    "    'GLUCOSE_mgdL',\n",
    "    'WBC_kuL',\n",
    "    'HCT_p',\n",
    "    'HB_gdL',\n",
    "    'PLT_kuL',\n",
    "    'ALP_UL',\n",
    "    'TBILI_mgdL',\n",
    "    'ALB_gdL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f93826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_cols = [\n",
    " 'BNP_VAL',\n",
    " 'age_entry',\n",
    " 'EF',\n",
    " 'visit_dt1_hf',\n",
    " 'chfdc_dt3',\n",
    " 'mi_dt3',\n",
    " 'stroke_dt3',\n",
    " 'cabg_dt3',\n",
    " 'pci_dt3',\n",
    " 'DM_AGE_YR',\n",
    " 'DM_DUR_YR',\n",
    " 'cigs',\n",
    " 'SMOKE_YRS',\n",
    " 'QUIT_YRS',\n",
    " 'HEAVY_MIN',\n",
    " 'HEAVY_WK',\n",
    " 'MED_WK',\n",
    " 'MED_MIN',\n",
    " 'LIGHT_WK',\n",
    " 'LIGHT_MIN',\n",
    " 'metsperweek',\n",
    " 'cooking_salt_score',\n",
    " 'height',\n",
    " 'weight',\n",
    " 'waistc',\n",
    " 'HR',\n",
    " 'SBP',\n",
    " 'DBP',\n",
    " 'CR_mgdl',\n",
    " 'gfr',\n",
    " 'labs_dt1',\n",
    " 'NA_mmolL',\n",
    " 'K_mmolL',\n",
    " 'CL_mmolL',\n",
    " 'BUN_mgdL',\n",
    " 'ALT_UL',\n",
    " 'AST_UL',\n",
    " 'urine_val_mgg',\n",
    " 'QRS_DUR',\n",
    " 'CR_mgdL',\n",
    " 'BMI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01358db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = pd.read_csv(\n",
    "    '/data/datasets/topcat/py_cleaned_data/TOPCAT_final_2_25_2020.csv',\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9321636",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "oversample = SMOTE()\n",
    "ran_undersample = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006fbd1",
   "metadata": {},
   "source": [
    "## Base Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0272afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Fold 0: XGB AUC=0.693089430894309\n",
      "\tRF AUC=0.7066960252935863\n",
      "Fold 1: XGB AUC=0.725609756097561\n",
      "\tRF AUC=0.7440718157181572\n",
      "Fold 2: XGB AUC=0.7172538392050587\n",
      "\tRF AUC=0.764284101174345\n",
      "Fold 3: XGB AUC=0.6824666359871145\n",
      "\tRF AUC=0.6852277956741831\n",
      "Fold 4: XGB AUC=0.6323055683387022\n",
      "\tRF AUC=0.6283364012885414\n",
      "\n",
      "XGB AUC=0.6901450461045491\n",
      "Avg RF AUC=0.7057232278297627\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c2ca0",
   "metadata": {},
   "source": [
    "## SMOTE on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e41083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Counter({1.0: 820, 0.0: 820})\n",
      "Fold 0: XGB AUC=0.7817519333729922\n",
      "\tRF AUC=0.8502193634741225\n",
      "Fold 1: XGB AUC=0.8196014277215943\n",
      "\tRF AUC=0.8437871802498513\n",
      "Fold 2: XGB AUC=0.9991448542534206\n",
      "\tRF AUC=1.0\n",
      "Fold 3: XGB AUC=0.9979550862581796\n",
      "\tRF AUC=0.9993679357525282\n",
      "Fold 4: XGB AUC=0.9958729922665079\n",
      "\tRF AUC=0.9985871505056515\n",
      "\n",
      "XGB AUC=0.9188652587745387\n",
      "Avg RF AUC=0.9383923259964307\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        df = df.fillna(df.mean()) # oversample cant take NaN values\n",
    "        # Oversampling the minority class in the entire dataset\n",
    "        df, labels = oversample.fit_resample(df, labels)\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62617c3",
   "metadata": {},
   "source": [
    "## SMOTE and Random Under Sampling in combo on entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0e3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Counter({0.0: 820, 1.0: 820})\n",
      "Fold 0: XGB AUC=0.9140020820939917\n",
      "\tRF AUC=0.9316627007733492\n",
      "Fold 1: XGB AUC=0.9390243902439025\n",
      "\tRF AUC=0.9440809042236764\n",
      "Fold 2: XGB AUC=0.940288518738846\n",
      "\tRF AUC=0.961220999405116\n",
      "Fold 3: XGB AUC=0.9312908982748365\n",
      "\tRF AUC=0.9292273944080904\n",
      "Fold 4: XGB AUC=0.8782718619869125\n",
      "\tRF AUC=0.8943709101725164\n",
      "\n",
      "XGB AUC=0.9205755502676979\n",
      "Avg RF AUC=0.9321125817965497\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        df = df.fillna(df.mean()) # oversample cant take NaN values\n",
    "        # Oversampling the minority class\n",
    "        df, labels = oversample.fit_resample(df, labels)\n",
    "        # Undersampling the majority class \n",
    "        df, labels = ran_undersample.fit_resample(df, labels)\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa0c01",
   "metadata": {},
   "source": [
    "## SMOTE on K-Fold Groups training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976d3f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Fold 0: Counter({1.0: 656, 0.0: 656})\n",
      "XGB AUC=0.7203026196928637\n",
      "\tRF AUC=0.7162375790424571\n",
      "Fold 1: Counter({1.0: 656, 0.0: 656})\n",
      "XGB AUC=0.6988482384823849\n",
      "\tRF AUC=0.7571702800361337\n",
      "Fold 2: Counter({1.0: 656, 0.0: 656})\n",
      "XGB AUC=0.6659891598915988\n",
      "\tRF AUC=0.6892502258355917\n",
      "Fold 3: Counter({1.0: 656, 0.0: 656})\n",
      "XGB AUC=0.6764841233317993\n",
      "\tRF AUC=0.63466405890474\n",
      "Fold 4: Counter({1.0: 656, 0.0: 656})\n",
      "XGB AUC=0.6852277956741832\n",
      "\tRF AUC=0.6547975149562817\n",
      "\n",
      "XGB AUC=0.689370387414566\n",
      "Avg RF AUC=0.6904239317550409\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            #---------------------------------------------------------------------------\n",
    "            train_data, train_labels = oversample.fit_resample(train_data, train_labels)\n",
    "            counter = Counter(train_labels)\n",
    "            print(counter)\n",
    "            #---------------------------------------------------------------------------\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc9c17",
   "metadata": {},
   "source": [
    "## SMOTE and Random Under Sampling in combo on k-fold groups training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "882b313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Fold 0: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.6666666666666666\n",
      "\tRF AUC=0.7015018066847335\n",
      "Fold 1: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.7125112917795844\n",
      "\tRF AUC=0.7406842818428183\n",
      "Fold 2: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.6903794037940378\n",
      "\tRF AUC=0.7169150858175249\n",
      "Fold 3: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.6505982512655315\n",
      "\tRF AUC=0.6410492406810862\n",
      "Fold 4: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.6587666820064427\n",
      "\tRF AUC=0.6663598711458814\n",
      "\n",
      "XGB AUC=0.6757844591024527\n",
      "Avg RF AUC=0.693302057234409\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            #---------------------------------------------------------------------------\n",
    "            train_data, train_labels = oversample.fit_resample(train_data, train_labels)\n",
    "            train_data, train_labels = ran_undersample.fit_resample(train_data, train_labels)\n",
    "            counter = Counter(train_labels)\n",
    "            print(counter)\n",
    "            #---------------------------------------------------------------------------\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef9b90",
   "metadata": {},
   "source": [
    "## SMOTE on K-Fold Groups training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "208ce8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Fold 0: XGB AUC=0.878457763236169\n",
      "\tRF AUC=0.8989069006543724\n",
      "Fold 1: XGB AUC=0.9048929208804283\n",
      "\tRF AUC=0.9014351576442593\n",
      "Fold 2: XGB AUC=0.8782718619869124\n",
      "\tRF AUC=0.8897233789411064\n",
      "Fold 3: XGB AUC=0.8645523497917905\n",
      "\tRF AUC=0.869125520523498\n",
      "Fold 4: XGB AUC=0.8838117192147531\n",
      "\tRF AUC=0.8827149018441404\n",
      "\n",
      "XGB AUC=0.8819973230220105\n",
      "Avg RF AUC=0.8883811719214754\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            #---------------------------------------------------------------------------\n",
    "            train_data, train_labels = oversample.fit_resample(train_data, train_labels)\n",
    "            test_data, test_labels = oversample.fit_resample(test_data, test_labels)\n",
    "            #---------------------------------------------------------------------------\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75911c8a",
   "metadata": {},
   "source": [
    "## SMOTE and Random Under Sampling in combo on k-fold groups training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a45224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode=2, Cut Off=3\n",
      "Counter({0.0: 820, 1.0: 268})\n",
      "Fold 0: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.8853361094586555\n",
      "\tRF AUC=0.8998735871505057\n",
      "Fold 1: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.8844437834622249\n",
      "\tRF AUC=0.9135745092207019\n",
      "Fold 2: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.8885707911957168\n",
      "\tRF AUC=0.8806699881023201\n",
      "Fold 3: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.8576368233194527\n",
      "\tRF AUC=0.8641247769185009\n",
      "Fold 4: Counter({0.0: 656, 1.0: 656})\n",
      "XGB AUC=0.8816924449732302\n",
      "\tRF AUC=0.8898349196906603\n",
      "\n",
      "XGB AUC=0.8795359904818559\n",
      "Avg RF AUC=0.8896155562165378\n"
     ]
    }
   ],
   "source": [
    "xgb_auc = []\n",
    "rf_auc = []\n",
    "#Modes:\n",
    "#     1 : Primary End Point\n",
    "#     2 : Death\n",
    "#     3 : HF Hospitalization\n",
    "\n",
    "#Cutoff: Years before censoring\n",
    "\n",
    "#for mode in range(1,4):\n",
    "#    for cutoff in [3, np.inf]:\n",
    "for mode in [2]:\n",
    "    for cutoff in [3]:\n",
    "        print(f'Mode={mode}, Cut Off={cutoff}')\n",
    "        df = base_data.copy()\n",
    "        \n",
    "        #remove people\n",
    "        if cutoff==3:\n",
    "            ids = pd.read_csv('/data/datasets/topcat/nch/Pt_ID.csv')\n",
    "            if mode == 2:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['Died_Americas_3years']) |\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "            elif mode == 3:\n",
    "                df = df[\n",
    "                    df['ID'].isin(ids['hosp_americas_3years']) |\n",
    "                    df['ID'].isin(ids['FU_alive_3y_Americas'])\n",
    "                ]\n",
    "        \n",
    "        #remove variables\n",
    "        df.drop(columns=df.columns[114:173], inplace=True)\n",
    "        df.drop(columns=['BNP_VAL', 'BNP_YN', 'visit_dt1_hf',], inplace=True)\n",
    "        df.drop(columns=late_drops, inplace=True)\n",
    "        \n",
    "        highly_miss = []\n",
    "        for col in df.columns:\n",
    "            if df[col].count()/df.shape[0] < 0.5:\n",
    "                highly_miss.append(col)\n",
    "        df.drop(columns=highly_miss, inplace=True)\n",
    "        \n",
    "        #create labels\n",
    "        if mode == 1:\n",
    "            outcome = 'primary_ep'\n",
    "            outcome_time = 'time_primary_ep'\n",
    "        elif mode == 2:\n",
    "            outcome = 'death'\n",
    "            outcome_time = 'time_death'\n",
    "        elif mode == 3:\n",
    "            outcome = 'hfhosp'\n",
    "            outcome_time = 'time_hfhosp'\n",
    "        \n",
    "        labels = df[outcome].copy()\n",
    "        complete_labels = labels.copy()\n",
    "        \n",
    "        labels.loc[df[outcome_time] > cutoff] = 0\n",
    "        #---------------------------------------------------------------------------\n",
    "        counter = Counter(labels)\n",
    "        print(counter)\n",
    "        #---------------------------------------------------------------------------\n",
    "        for i, (train, test) in enumerate(skf.split(df, labels)):\n",
    "            print(f'Fold {i}: ', end='')\n",
    "            train_data = df.iloc[train].copy()\n",
    "            test_data = df.iloc[test].copy()\n",
    "            \n",
    "            train_labels=labels.iloc[train].copy()\n",
    "            test_labels=labels.iloc[test].copy()\n",
    "            \n",
    "            weights = len(train_labels)/test_labels.sum()\n",
    "            glm_weights = pd.Series(data=1, index=train_labels.index)\n",
    "            glm_weights.loc[train_labels==1] = weights\n",
    "            \n",
    "            #remove outcomes\n",
    "            train_id = train_data['ID'].copy()\n",
    "            test_id = test_data['ID'].copy()\n",
    "            \n",
    "            train_data.drop(columns=outcome_cols+['ID'], inplace=True)\n",
    "            test_data.drop(columns= outcome_cols+['ID'], inplace=True)\n",
    "            \n",
    "            #print(f'Fold {i} Imputation')\n",
    "            #imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            #train_data.values = imp.fit_transform(train_data)\n",
    "            #test_data.values  = imp.transform(test_data)\n",
    "            test_data = test_data.fillna(train_data.mean())\n",
    "            train_data = train_data.fillna(train_data.mean())\n",
    "            \n",
    "            #---------------------------------------------------------------------------\n",
    "            train_data, train_labels = oversample.fit_resample(train_data, train_labels)\n",
    "            train_data, train_labels = ran_undersample.fit_resample(train_data, train_labels)\n",
    "            test_data, test_labels = oversample.fit_resample(test_data, test_labels)\n",
    "            test_data, test_labels = ran_undersample.fit_resample(test_data, test_labels)\n",
    "\n",
    "            counter = Counter(train_labels)\n",
    "            print(counter)\n",
    "            #---------------------------------------------------------------------------\n",
    "            \n",
    "            sd_0_cols = train_data.columns[(train_data.std() == 0)]\n",
    "            train_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            test_data.drop(columns=sd_0_cols, inplace=True)\n",
    "            \n",
    "            cols_to_scale = [foo for foo in con_cat_cols + contin_cols if foo in train_data.columns]\n",
    "            scaler = StandardScaler()\n",
    "            train_data.loc[:,cols_to_scale] = scaler.fit_transform(train_data.loc[:,cols_to_scale])\n",
    "            test_data.loc[:,cols_to_scale]  = scaler.transform(test_data.loc[:,cols_to_scale])\n",
    "            \n",
    "            xgb = XGBClassifier(use_label_encoder=False, verbosity = 0)\n",
    "            xgb.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, xgb.predict_proba(test_data)[:,1])\n",
    "            print(f'XGB AUC={auc}')\n",
    "            xgb_auc.append(auc)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(train_data, train_labels)\n",
    "            auc = roc_auc_score(test_labels, rf.predict_proba(test_data)[:,1])\n",
    "            print(f'\\tRF AUC={auc}')\n",
    "            rf_auc.append(auc)\n",
    "            \n",
    "            \n",
    "            #print('\\tSuccess!\\n')\n",
    "            #break\n",
    "        print()\n",
    "        print(f'Avg XGB AUC={sum(xgb_auc)/5}')\n",
    "        print(f'Avg RF AUC={sum(rf_auc)/5}')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
